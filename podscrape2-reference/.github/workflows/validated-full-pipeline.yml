name: Validated Full Pipeline

on:
  schedule:
    - cron: '0 5 * * *'  # Daily at 5:00 AM UTC
  workflow_dispatch:
    inputs:
      dry_run:
        description: "Enable dry run mode (true/false)"
        required: false
        default: "false"

concurrency:
  group: phase-tts-${{ github.ref }}
  cancel-in-progress: true

jobs:
  tts:
    name: Run Validated Pipeline
    runs-on: ubuntu-latest

    permissions:
      contents: write    # For git push operations
      packages: write    # For GitHub releases

    env:
      PYTHON_VERSION: "3.13"
      VENV_PATH: .venv
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
      GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
      GITHUB_REPOSITORY: ${{ github.repository }}
      DRY_RUN: ${{ inputs.dry_run }}
      PIPELINE_RUN_ID: ${{ format('{0}-{1}', github.run_id, github.run_attempt) }}
      PIPELINE_WORKFLOW_NAME: ${{ github.workflow }}
      PIPELINE_TRIGGER: ${{ github.event_name }}
      PIPELINE_DB_LOGS: "true"  # Batch logging enabled (v1.66) - writes at phase completion

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure git for commits
        run: |
          git config --global user.name "Paul Brown"
          git config --global user.email "brownpr0@gmail.com"

      - name: Ensure required secrets present
        run: |
          missing=()
          for var in DATABASE_URL OPENAI_API_KEY GITHUB_TOKEN GITHUB_REPOSITORY; do
            if [ -z "${!var}" ]; then
              missing+=("$var")
            fi
          done
          if [ ${#missing[@]} -gt 0 ]; then
            echo "Missing secrets: ${missing[*]}" >&2
            exit 1
          fi

      - name: Free up disk space
        run: |
          echo "Disk space before cleanup:"
          df -h

          # Remove unnecessary software to free up ~10GB
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL

          # Clean apt cache
          sudo apt-get clean

          # Remove docker images
          docker rmi $(docker images -q) -f 2>/dev/null || true

          # Clean up old data files that may have accumulated
          rm -rf data/completed-tts/*.mp3 2>/dev/null || true
          rm -rf data/transcripts/*.txt 2>/dev/null || true
          rm -rf data/scripts/*.md 2>/dev/null || true

          echo "Disk space after cleanup:"
          df -h

      - name: Install system dependencies
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip downloads
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ env.PYTHON_VERSION }}-
            pip-${{ runner.os }}-

      - name: Cache virtual environment
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            venv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-
            venv-${{ runner.os }}-

      - name: Install dependencies
        run: |
          if [ ! -d "${VENV_PATH}" ]; then
            python -m venv "${VENV_PATH}"
          fi
          source "${VENV_PATH}/bin/activate"
          python -m pip install --upgrade pip
          # Install torch with CPU-only index to avoid CUDA bloat (~2GB saved)
          python -m pip install torch --index-url https://download.pytorch.org/whl/cpu
          # Use regular requirements (disk cleanup step freed up ~10GB)
          python -m pip install -r requirements.txt

      - name: Phase 1 - Discovery
        run: |
          source "${VENV_PATH}/bin/activate"
          mkdir -p artifacts

          echo "Running: python scripts/run_discovery.py --verbose --output artifacts/discovery-output.json"
          python scripts/run_discovery.py --verbose --output artifacts/discovery-output.json
        env:
          ORCHESTRATED_EXECUTION: "1"

      - name: Phase 2 - Audio Processing
        run: |
          source "${VENV_PATH}/bin/activate"
          mkdir -p artifacts

          echo "Running: python scripts/run_audio.py --verbose --output artifacts/audio-output.json"
          python scripts/run_audio.py --verbose --output artifacts/audio-output.json
        env:
          ORCHESTRATED_EXECUTION: "1"

      # Audio transcripts are now stored in database - no git commits needed

      - name: Phase 3 - Digest
        run: |
          source "${VENV_PATH}/bin/activate"
          mkdir -p artifacts
          python scripts/run_digest.py \
            --verbose \
            --output artifacts/digest-output.json
        env:
          ORCHESTRATED_EXECUTION: "1"

      # Digest scripts are now stored in database - no git commits needed

      - name: Capture existing MP3 inventory
        id: capture_mp3_baseline
        run: |
          set -euo pipefail
          TARGET_BRANCH=${TARGET_BRANCH:-main}
          source "${VENV_PATH}/bin/activate"
          mkdir -p data/completed-tts
          mkdir -p artifacts/tts-persist
          BASELINE=artifacts/tts-persist/before.txt
          find data/completed-tts -maxdepth 1 -type f -name '*.mp3' -print | sort > "$BASELINE" || true
          echo "before_file=$(realpath "$BASELINE")" >> "$GITHUB_OUTPUT"

      - name: Phase 4 - TTS Audio Generation
        run: |
          source "${VENV_PATH}/bin/activate"
          mkdir -p artifacts
          python scripts/run_tts.py \
            --verbose \
            --output artifacts/tts-output.json
        env:
          ORCHESTRATED_EXECUTION: "1"

      # Phase 5: Publishing - Upload MP3s to GitHub and update database
      # ARCHITECTURE NOTE (v1.49): RSS feed is now generated dynamically by Next.js API!
      #   - API route queries database directly: web_ui_hosted/app/api/rss/daily-digest/route.ts
      #   - Public URL: https://podcast.paulrbrown.org/daily-digest.xml (via vercel.json rewrite)
      #   - Benefits: Instant updates (no git commits), always accurate (reads from database)
      #   - This phase only: uploads MP3s to GitHub, updates database status
      #   - No RSS file generation, no git commits, no Vercel deployment wait needed
      - name: Phase 5 - Publishing
        if: success() && inputs.dry_run != 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          TARGET_BRANCH: ${{ github.ref_name }}
          BASELINE_FILE: ${{ steps.capture_mp3_baseline.outputs.before_file }}
        run: |
          set -euo pipefail
          mkdir -p artifacts/tts-persist
          mkdir -p data/completed-tts

          BASELINE=${BASELINE_FILE:-}
          if [ -z "$BASELINE" ] || [ ! -f "$BASELINE" ]; then
            BASELINE=$(mktemp)
            touch "$BASELINE"
          fi

          AFTER=$(mktemp)
          find data/completed-tts -maxdepth 1 -type f -name '*.mp3' -print | sort > "$AFTER"

          NEW_LIST=artifacts/tts-persist/new_files.txt
          comm -13 "$BASELINE" "$AFTER" > "$NEW_LIST"

          if [ ! -s "$NEW_LIST" ]; then
            echo "No new MP3 files detected; skipping persistence."
            exit 0
          fi

          mapfile -t FILES < "$NEW_LIST"
          PRIMARY_FILE=${FILES[0]}
          BASENAME=$(basename "$PRIMARY_FILE")
          DATE_PART=$(echo "$BASENAME" | sed -E 's/.*_([0-9]{8})_[0-9]{6}\.mp3/\1/')
          if [ -z "$DATE_PART" ]; then
            DATE_PART=$(date -u +%Y%m%d)
          fi
          RELEASE_DATE="${DATE_PART:0:4}-${DATE_PART:4:2}-${DATE_PART:6:2}"

          source "${VENV_PATH}/bin/activate"

          echo "Publishing MP3 files to GitHub Release:"
          echo "  Release date: $RELEASE_DATE"
          echo "  Files to publish:"
          for file in "${FILES[@]}"; do
            echo "    - $file ($([ -f "$file" ] && echo "exists" || echo "MISSING"))"
          done

          echo "Creating GitHub Release and uploading MP3 assets..."
          echo "Command: python scripts/publish_release_assets.py --publish-date \"$RELEASE_DATE\" ${FILES[*]}"

          if ! python scripts/publish_release_assets.py --verbose --publish-date "$RELEASE_DATE" "${FILES[@]}"; then
            echo "ERROR: Failed to create GitHub Release and upload MP3 files"
            echo "Checking for any GitHub releases created today..."
            gh release list --repo "$GITHUB_REPOSITORY" --limit 5 || echo "Failed to list releases"
            exit 1
          fi

          echo "Verifying GitHub Release was created..."
          sleep 5  # Give GitHub API time to propagate

          echo "Running RSS publishing (uploads MP3s to GitHub, updates database)..."
          python scripts/run_publishing.py --verbose

          echo "âœ… Publishing complete!"
          echo "ðŸ“¡ RSS feed is generated dynamically by Next.js API route"
          echo "ðŸŒ Feed URL: https://podcast.paulrbrown.org/daily-digest.xml"
          echo "âš¡ Episodes served directly from database with edge caching"
          echo ""
          echo "Verifying RSS feed API is accessible..."
          if curl -s https://podcast.paulrbrown.org/daily-digest.xml | head -20 | grep -q "<rss"; then
            echo "âœ… RSS feed API responding successfully"
          else
            echo "âš ï¸  RSS feed API may need time to propagate or cache to clear"
          fi

      # Phase 6: Retention Management
      # Cleans up old files, database records, and GitHub releases
      # Retention policies are configured via web UI (web_settings table):
      #   - local_mp3_days: Local MP3 files (default: 14 days)
      #   - audio_cache_days: Audio cache files (default: 3 days)
      #   - logs_days: Log files (default: 3 days)
      #   - github_release_days: GitHub releases (default: 14 days)
      #   - episode_retention_days: Episode database records (default: 14 days)
      #   - digest_retention_days: Digest database records (default: 14 days)
      - name: Phase 6 - Retention Management
        run: |
          source "${VENV_PATH}/bin/activate"
          echo "Running: python scripts/run_retention.py --verbose"
          python scripts/run_retention.py --verbose
        env:
          ORCHESTRATED_EXECUTION: "1"

      # Logs are now persisted in the database via PipelineLogRepository
