"""
TopicExtractor: Extracts story arcs from episode transcripts using GPT.

Story arcs are evolving narratives that track news stories over time.
Each episode may introduce new story arcs or add events to existing ones.

Key features:
- LLM-driven story arc recognition (no hardcoded keywords)
- Multiple perspectives from different feeds captured as events
- Timeline tracking shows story evolution
- Functional category classification for organization
"""

import os
import json
import logging
import re
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

# Functional categories for story arc classification
FUNCTIONAL_CATEGORIES = [
    "model_release",      # New model announcements, updates, versions
    "company_strategy",   # Business moves, pivots, leadership changes
    "research",           # Papers, studies, breakthroughs
    "regulation",         # Policy, legal, governance
    "product_launch",     # New products, features, services
    "partnership",        # Collaborations, acquisitions, investments
    "controversy",        # Disputes, criticisms, debates
    "industry_trend",     # Broader patterns, market shifts
    "technique",          # New methods, approaches, architectures
    "use_case",           # Applications, implementations
    "other"               # Miscellaneous
]


class StoryArcExtractor:
    """
    Extracts and tracks story arcs from episode transcripts.

    Story arcs are ongoing narratives (e.g., "OpenAI's GPT-5 Development")
    that evolve over time with events from multiple sources.
    """

    def __init__(
        self,
        db_client,
        max_arcs_per_episode: int = 10,
    ):
        """
        Initialize StoryArcExtractor.

        Args:
            db_client: Database client with story arc methods
            max_arcs_per_episode: Maximum story arcs to extract per episode
        """
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")

        self.client = OpenAI(api_key=api_key, timeout=120.0)
        self.db = db_client
        self.max_arcs_per_episode = max_arcs_per_episode

        # Get model from web_settings, fallback to default
        try:
            self.model = db_client.get_setting(
                'topic_tracking', 'extraction_model', 'gpt-4o-mini'
            )
            logger.info(f"Using extraction model: {self.model}")
        except Exception as e:
            self.model = "gpt-4o-mini"
            logger.warning(f"Failed to get model from settings, using: {self.model}")

    def extract_and_store_story_arcs(
        self,
        episode_id: int,
        episode_guid: str,
        feed_id: int,
        digest_topic: str,
        transcript: str,
        episode_title: str,
        episode_published_date: datetime,
        relevance_score: float = 0.0,
    ) -> List[Dict]:
        """
        Extract story arcs from transcript and store in database.

        The LLM receives context about active story arcs and decides:
        1. Which existing arcs this content continues
        2. What new arcs this content introduces

        Args:
            episode_id: Episode database ID
            episode_guid: Episode GUID
            feed_id: Source feed ID
            digest_topic: Parent topic (e.g., "AI and Technology")
            transcript: Full episode transcript
            episode_title: Episode title (for source attribution)
            episode_published_date: When episode was published
            relevance_score: Episode's relevance score

        Returns:
            List of story arc results (new arcs and events added)
        """
        logger.info(
            f"Extracting story arcs from episode {episode_guid} for {digest_topic}"
        )

        # Get active story arcs for context
        active_arcs_context = ""
        try:
            active_arcs_context = self.db.get_story_arcs_for_prompt(
                digest_topic=digest_topic,
                max_arcs=20,
                max_events_per_arc=4
            )
            arc_count = active_arcs_context.count("STORY ARC") if active_arcs_context else 0
            logger.info(f"Retrieved {arc_count} active story arcs for context")
        except Exception as e:
            logger.warning(f"Failed to retrieve active story arcs: {e}")

        # Create prompt for GPT
        prompt = self._create_extraction_prompt(
            transcript=transcript,
            digest_topic=digest_topic,
            active_arcs_context=active_arcs_context,
            episode_title=episode_title
        )
        schema = self._create_extraction_schema()

        try:
            # Call GPT with structured output
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "story_arc_extraction",
                        "schema": schema,
                        "strict": True,
                    },
                },
                max_tokens=3000,
            )

            # Parse response
            extraction_data = json.loads(response.choices[0].message.content)

            # Process continuing arcs (updates to existing stories)
            continuing_arcs = extraction_data.get("continuing_arcs", [])
            new_arcs = extraction_data.get("new_arcs", [])

            logger.info(
                f"Extracted {len(continuing_arcs)} continuing arcs, "
                f"{len(new_arcs)} new arcs from {episode_guid}"
            )

            results = []

            # Handle continuing arcs (add events to existing stories)
            for arc_data in continuing_arcs[:self.max_arcs_per_episode]:
                try:
                    arc_name = arc_data["arc_name"]
                    event_summary = arc_data["event_summary"]
                    key_points = arc_data.get("key_points", [])
                    perspective = arc_data.get("perspective")

                    # Find or get the existing arc
                    arc = self.db.get_or_create_story_arc(
                        arc_name=arc_name,
                        digest_topic=digest_topic,
                        functional_category=arc_data.get("category", "other")
                    )

                    # Add the new event
                    event = self.db.add_story_arc_event(
                        story_arc_id=arc['id'],
                        event_date=episode_published_date,
                        event_summary=event_summary,
                        key_points=key_points,
                        source_feed_id=feed_id,
                        source_episode_id=episode_id,
                        source_episode_guid=episode_guid,
                        source_name=episode_title,
                        perspective=perspective,
                        relevance_score=relevance_score
                    )

                    results.append({
                        "arc_name": arc_name,
                        "arc_id": arc['id'],
                        "is_new": False,
                        "event_id": event['id'],
                        "event_summary": event_summary
                    })

                    logger.info(
                        f"Added event to story arc '{arc_name}' (id={arc['id']})"
                    )

                except Exception as e:
                    logger.warning(
                        f"Failed to add event to arc '{arc_data.get('arc_name', 'unknown')}': {e}"
                    )

            # Handle new arcs (create new stories)
            for arc_data in new_arcs[:self.max_arcs_per_episode - len(results)]:
                try:
                    arc_name = arc_data["arc_name"]
                    event_summary = arc_data["event_summary"]
                    key_points = arc_data.get("key_points", [])
                    category = arc_data.get("category", "other")
                    perspective = arc_data.get("perspective")

                    # Create the arc with initial event
                    arc = self.db.create_story_arc(
                        arc_name=arc_name,
                        digest_topic=digest_topic,
                        functional_category=category,
                        initial_event={
                            "event_date": episode_published_date,
                            "event_summary": event_summary,
                            "key_points": key_points,
                            "source_feed_id": feed_id,
                            "source_episode_id": episode_id,
                            "source_episode_guid": episode_guid,
                            "source_name": episode_title,
                            "perspective": perspective,
                            "relevance_score": relevance_score
                        }
                    )

                    results.append({
                        "arc_name": arc_name,
                        "arc_id": arc['id'],
                        "is_new": True,
                        "category": category,
                        "event_summary": event_summary
                    })

                    logger.info(
                        f"Created new story arc '{arc_name}' (id={arc['id']}, category={category})"
                    )

                except Exception as e:
                    logger.warning(
                        f"Failed to create arc '{arc_data.get('arc_name', 'unknown')}': {e}"
                    )

            logger.info(
                f"Episode {episode_guid}: {len([r for r in results if r['is_new']])} new arcs, "
                f"{len([r for r in results if not r['is_new']])} arcs updated"
            )

            return results

        except Exception as e:
            logger.error(f"Story arc extraction failed for {episode_guid}: {e}")
            raise

    def _create_extraction_prompt(
        self,
        transcript: str,
        digest_topic: str,
        active_arcs_context: str,
        episode_title: str
    ) -> str:
        """
        Create GPT prompt for story arc extraction.

        Args:
            transcript: Episode transcript
            digest_topic: Parent topic name
            active_arcs_context: Formatted active story arcs
            episode_title: Episode title for context

        Returns:
            Formatted prompt string
        """
        # Truncate transcript to reasonable length
        truncated_transcript = transcript[:6000]

        active_arcs_section = ""
        if active_arcs_context:
            active_arcs_section = f"""
## ACTIVE STORY ARCS
The following stories are currently being tracked. If this episode discusses any of these stories,
you should add a NEW EVENT to that story arc rather than creating a duplicate.

{active_arcs_context}

---
"""

        return f"""Analyze this podcast episode transcript and identify STORY ARCS related to "{digest_topic}".

A STORY ARC is an ongoing news narrative that evolves over time. Examples:
- "OpenAI's GPT-5 Development" (tracks rumors → announcements → release → reactions)
- "EU AI Act Implementation" (tracks drafts → votes → enforcement → industry response)
- "Google Gemini Launch" (tracks leaks → announcement → reviews → updates)

{active_arcs_section}

## YOUR TASK

For this episode from "{episode_title}", identify:

1. **CONTINUING ARCS**: Stories from the active list above that this episode discusses
   - Add a NEW EVENT capturing what this episode says about the story
   - Capture the episode's PERSPECTIVE (positive, negative, neutral, analytical)
   - Include 2-3 specific key points from this episode

2. **NEW ARCS**: New stories not in the active list
   - Only create if this is a significant, newsworthy development
   - Don't create arcs for general discussion topics (too broad)
   - Each arc should be specific enough to track over time

## CLASSIFICATION CATEGORIES
Use one of these for each arc:
- model_release: New model announcements, updates, versions
- company_strategy: Business moves, pivots, leadership changes
- research: Papers, studies, breakthroughs
- regulation: Policy, legal, governance
- product_launch: New products, features, services
- partnership: Collaborations, acquisitions, investments
- controversy: Disputes, criticisms, debates
- industry_trend: Broader patterns, market shifts
- technique: New methods, approaches, architectures
- use_case: Applications, implementations
- other: Miscellaneous

## PERSPECTIVE VALUES
- positive: Episode is enthusiastic/supportive about this development
- negative: Episode is critical/concerned about this development
- neutral: Episode presents factual coverage without strong stance
- analytical: Episode provides in-depth analysis/comparison

## TRANSCRIPT
{truncated_transcript}

---
Identify story arcs and events from this episode."""

    def _create_extraction_schema(self) -> dict:
        """
        JSON schema for structured story arc extraction.

        Returns:
            JSON schema dictionary
        """
        arc_event_schema = {
            "type": "object",
            "properties": {
                "arc_name": {
                    "type": "string",
                    "description": "Name of the story arc (use existing name if continuing)"
                },
                "event_summary": {
                    "type": "string",
                    "description": "1-2 sentence summary of what this episode says about the story"
                },
                "key_points": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1,
                    "maxItems": 4,
                    "description": "Specific details from this episode"
                },
                "category": {
                    "type": "string",
                    "enum": FUNCTIONAL_CATEGORIES,
                    "description": "Functional category of the story"
                },
                "perspective": {
                    "type": "string",
                    "enum": ["positive", "negative", "neutral", "analytical"],
                    "description": "Episode's perspective on this story"
                }
            },
            "required": ["arc_name", "event_summary", "key_points", "category", "perspective"],
            "additionalProperties": False
        }

        return {
            "type": "object",
            "properties": {
                "continuing_arcs": {
                    "type": "array",
                    "items": arc_event_schema,
                    "description": "Events for existing story arcs"
                },
                "new_arcs": {
                    "type": "array",
                    "items": arc_event_schema,
                    "description": "New story arcs introduced by this episode"
                }
            },
            "required": ["continuing_arcs", "new_arcs"],
            "additionalProperties": False
        }


# Backwards compatibility alias
class TopicExtractor(StoryArcExtractor):
    """
    Backwards-compatible alias for StoryArcExtractor.

    The old TopicExtractor extracted topics; the new StoryArcExtractor
    extracts story arcs. This alias allows existing code to work.
    """

    def __init__(
        self,
        db_client,
        max_topics: int = 15,
        novelty_threshold: float = 0.30,
        enable_novelty_detection: bool = True,
        semantic_similarity_threshold: float = 0.80
    ):
        # Ignore old parameters, use new defaults
        super().__init__(
            db_client=db_client,
            max_arcs_per_episode=max_topics
        )
        logger.info(
            "TopicExtractor is now StoryArcExtractor - "
            "novelty_threshold and semantic_similarity_threshold are no longer used"
        )

    def extract_and_store_topics(
        self,
        episode_id: int,
        episode_guid: str,
        digest_topic: str,
        transcript: str,
        relevance_score: float,
    ) -> List[Dict]:
        """
        Backwards-compatible wrapper for extract_and_store_story_arcs.

        Note: This requires feed_id, episode_title, and episode_published_date
        which the old API didn't have. We'll try to get them from the database.
        """
        # Try to get episode details from database
        try:
            episode = self.db.get_episode_by_guid(episode_guid)
            if episode:
                feed_id = episode.get('feed_id')
                episode_title = episode.get('title', 'Unknown Episode')
                episode_published_date = episode.get('published_date', datetime.now(timezone.utc))
            else:
                feed_id = None
                episode_title = 'Unknown Episode'
                episode_published_date = datetime.now(timezone.utc)
        except Exception as e:
            logger.warning(f"Failed to get episode details: {e}")
            feed_id = None
            episode_title = 'Unknown Episode'
            episode_published_date = datetime.now(timezone.utc)

        # Call the new method
        results = self.extract_and_store_story_arcs(
            episode_id=episode_id,
            episode_guid=episode_guid,
            feed_id=feed_id,
            digest_topic=digest_topic,
            transcript=transcript,
            episode_title=episode_title,
            episode_published_date=episode_published_date,
            relevance_score=relevance_score
        )

        # Convert results to old format for compatibility
        return [
            {
                "name": r.get("arc_name"),
                "type": r.get("category", "other"),
                "key_points": [],  # Events don't have key_points in the same way
                "novelty_score": 1.0 if r.get("is_new") else 0.5,
                "matched_existing": not r.get("is_new"),
                "story_arc": r.get("arc_name")
            }
            for r in results
        ]
